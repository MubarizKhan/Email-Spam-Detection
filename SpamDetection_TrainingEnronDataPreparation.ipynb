{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MubarizKhan/Email-Spam-Detection/blob/main/SpamDetection_TrainingEnronDataPreparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw5ztzTN41Ru"
      },
      "source": [
        "## Dataset Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBaFzHQq2OXi"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    we will be performing the necessary data preprocessing & feature engineering processes in this process which are necessary for training a machine learning model to make \n",
        "    sense of that data.\n",
        "\n",
        "    I'm combining several of datasets, and this is dataset possesses several of inconsistencies so has a bit time consuming data-consuming.  \n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEvPNs-2sKjj",
        "outputId": "f88db96d-750e-4073-8534-cea91c018ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rluxDbBesid4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WQ2IKDKDZrQ",
        "outputId": "1f71cc5e-379a-4871-9e1b-6623381c3c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import wordcloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXVm0TE0BOmJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import metrics\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NntqwAz95w0U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hZVUqlxtyvb"
      },
      "source": [
        "**Required Tasks**\n",
        "    \n",
        "    We have to clean the data using regex, matching patterns in the e-mail messages, and replace them with more organized counterparts.\n",
        "    Cleaner data leads to a more efficient model and higher accuracy. Following steps are involved in pre-processing the messages :\n",
        "\n",
        "    1.) Replace email address\n",
        "    2.) Replace URLs\n",
        "    3.) Replace currency symbols\n",
        "    4.) Replace 10 digits phone numbers (formats include parenthesis, spaces, no spaces, dashes)\n",
        "    5.) Replace numeric characters\n",
        "    6.) Removing punctuation\n",
        "    7.) Replace whitespace between terms with a single space\n",
        "    8.) Remove leading and trailing whitespace\n",
        "\n",
        "    ----------------------------\n",
        "    These are the NLP pre-processing we will perform after combining our datasets. \n",
        "    \n",
        "    sentence segmentation\n",
        "    word tokenization\n",
        "    stemming\n",
        "    lemmatizing\n",
        "    removal of stopwords\n",
        "    dependency parsing\n",
        "    POS tags\n",
        "    Named Entity Recognition\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8nEMwyo5wxs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1CXiSIt4DSt",
        "outputId": "2096c376-427a-4d6e-aa72-016da874451d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/drive/MyDrive/Spam-mail-datasets/training_dataset_spammy/ham/')\n",
        "print(type(os.listdir()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ8adGs74q2k",
        "outputId": "68d21107-d2cd-4fd8-a189-6e24a0f65119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Spam-mail-datasets/training_dataset_spammy/ham\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndPdWhP94x2C",
        "outputId": "c3e11924-bf2c-42df-8145-04c3122806cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['269-kitchen-ham_files.txt', '121-kitchen-ham_files.txt', '163-kitchen-ham_files.txt', '186-kitchen-ham_files.txt', '223-kitchen-ham_files.txt', '32-kitchen-ham_files.txt', '234-kitchen-ham_files.txt', '179-kitchen-ham_files.txt', '220-kitchen-ham_files.txt', '287-kitchen-ham_files.txt', '28-kitchen-ham_files.txt', '260-kitchen-ham_files.txt', '93-kitchen-ham_files.txt', '70-kitchen-ham_files.txt', '66-kitchen-ham_files.txt', '105-kitchen-ham_files.txt', '39-kitchen-ham_files.txt', '241-kitchen-ham_files.txt', '203-kitchen-ham_files.txt', '240-kitchen-ham_files.txt', '10-kitchen-ham_files.txt', '64-kitchen-ham_files.txt', '245-kitchen-ham_files.txt', '190-kitchen-ham_files.txt', '283-kitchen-ham_files.txt', '197-kitchen-ham_files.txt', '221-kitchen-ham_files.txt', '97-kitchen-ham_files.txt', '177-kitchen-ham_files.txt', '79-kitchen-ham_files.txt', '233-kitchen-ham_files.txt', '170-kitchen-ham_files.txt', '21-kitchen-ham_files.txt', '191-kitchen-ham_files.txt', '273-kitchen-ham_files.txt', '183-kitchen-ham_files.txt', '228-kitchen-ham_files.txt', '187-kitchen-ham_files.txt', '172-kitchen-ham_files.txt', '230-kitchen-ham_files.txt', '61-kitchen-ham_files.txt', '65-kitchen-ham_files.txt', '113-kitchen-ham_files.txt', '166-kitchen-ham_files.txt', '208-kitchen-ham_files.txt', '126-kitchen-ham_files.txt', '295-kitchen-ham_files.txt', '67-kitchen-ham_files.txt', '31-kitchen-ham_files.txt', '147-kitchen-ham_files.txt', '154-kitchen-ham_files.txt', '77-kitchen-ham_files.txt', '36-kitchen-ham_files.txt', '134-kitchen-ham_files.txt', '63-kitchen-ham_files.txt', '141-kitchen-ham_files.txt', '290-kitchen-ham_files.txt', '69-kitchen-ham_files.txt', '293-kitchen-ham_files.txt', '182-kitchen-ham_files.txt', '48-kitchen-ham_files.txt', '96-kitchen-ham_files.txt', '15-kitchen-ham_files.txt', '102-kitchen-ham_files.txt', '226-kitchen-ham_files.txt', '18-kitchen-ham_files.txt', '251-kitchen-ham_files.txt', '101-kitchen-ham_files.txt', '125-kitchen-ham_files.txt', '185-kitchen-ham_files.txt', '71-kitchen-ham_files.txt', '58-kitchen-ham_files.txt', '143-kitchen-ham_files.txt', '144-kitchen-ham_files.txt', '9-kitchen-ham_files.txt', '82-kitchen-ham_files.txt', '213-kitchen-ham_files.txt', '216-kitchen-ham_files.txt', '248-kitchen-ham_files.txt', '178-kitchen-ham_files.txt', '229-kitchen-ham_files.txt', '161-kitchen-ham_files.txt', '8-kitchen-ham_files.txt', '288-kitchen-ham_files.txt', '29-kitchen-ham_files.txt', '214-kitchen-ham_files.txt', '188-kitchen-ham_files.txt', '252-kitchen-ham_files.txt', '40-kitchen-ham_files.txt', '12-kitchen-ham_files.txt', '296-kitchen-ham_files.txt', '4-kitchen-ham_files.txt', '278-kitchen-ham_files.txt', '150-kitchen-ham_files.txt', '140-kitchen-ham_files.txt', '200-kitchen-ham_files.txt', '300-kitchen-ham_files.txt', '238-kitchen-ham_files.txt', '217-kitchen-ham_files.txt', '52-kitchen-ham_files.txt', '135-kitchen-ham_files.txt', '167-kitchen-ham_files.txt', '26-kitchen-ham_files.txt', '118-kitchen-ham_files.txt', '282-kitchen-ham_files.txt', '199-kitchen-ham_files.txt', '128-kitchen-ham_files.txt', '272-kitchen-ham_files.txt', '198-kitchen-ham_files.txt', '107-kitchen-ham_files.txt', '110-kitchen-ham_files.txt', '98-kitchen-ham_files.txt', '115-kitchen-ham_files.txt', '284-kitchen-ham_files.txt', '139-kitchen-ham_files.txt', '16-kitchen-ham_files.txt', '218-kitchen-ham_files.txt', '193-kitchen-ham_files.txt', '266-kitchen-ham_files.txt', '47-kitchen-ham_files.txt', '261-kitchen-ham_files.txt', '180-kitchen-ham_files.txt', '108-kitchen-ham_files.txt', '247-kitchen-ham_files.txt', '258-kitchen-ham_files.txt', '171-kitchen-ham_files.txt', '151-kitchen-ham_files.txt', '235-kitchen-ham_files.txt', '291-kitchen-ham_files.txt', '130-kitchen-ham_files.txt', '149-kitchen-ham_files.txt', '212-kitchen-ham_files.txt', '81-kitchen-ham_files.txt', '75-kitchen-ham_files.txt', '181-kitchen-ham_files.txt', '34-kitchen-ham_files.txt', '215-kitchen-ham_files.txt', '145-kitchen-ham_files.txt', '7-kitchen-ham_files.txt', '60-kitchen-ham_files.txt', '242-kitchen-ham_files.txt', '112-kitchen-ham_files.txt', '152-kitchen-ham_files.txt', '19-kitchen-ham_files.txt', '132-kitchen-ham_files.txt', '45-kitchen-ham_files.txt', '204-kitchen-ham_files.txt', '104-kitchen-ham_files.txt', '142-kitchen-ham_files.txt', '30-kitchen-ham_files.txt', '127-kitchen-ham_files.txt', '254-kitchen-ham_files.txt', '24-kitchen-ham_files.txt', '146-kitchen-ham_files.txt', '297-kitchen-ham_files.txt', '257-kitchen-ham_files.txt', '46-kitchen-ham_files.txt', '11-kitchen-ham_files.txt', '54-kitchen-ham_files.txt', '137-kitchen-ham_files.txt', '286-kitchen-ham_files.txt', '207-kitchen-ham_files.txt', '6-kitchen-ham_files.txt', '138-kitchen-ham_files.txt', '263-kitchen-ham_files.txt', '89-kitchen-ham_files.txt', '202-kitchen-ham_files.txt', '164-kitchen-ham_files.txt', '100-kitchen-ham_files.txt', '106-kitchen-ham_files.txt', '59-kitchen-ham_files.txt', '155-kitchen-ham_files.txt', '275-kitchen-ham_files.txt', '73-kitchen-ham_files.txt', '74-kitchen-ham_files.txt', '159-kitchen-ham_files.txt', '3-kitchen-ham_files.txt', '122-kitchen-ham_files.txt', '153-kitchen-ham_files.txt', '131-kitchen-ham_files.txt', '41-kitchen-ham_files.txt', '42-kitchen-ham_files.txt', '17-kitchen-ham_files.txt', '162-kitchen-ham_files.txt', '62-kitchen-ham_files.txt', '133-kitchen-ham_files.txt', '123-kitchen-ham_files.txt', '222-kitchen-ham_files.txt', '196-kitchen-ham_files.txt', '27-kitchen-ham_files.txt', '53-kitchen-ham_files.txt', '37-kitchen-ham_files.txt', '201-kitchen-ham_files.txt', '294-kitchen-ham_files.txt', '86-kitchen-ham_files.txt', '49-kitchen-ham_files.txt', '210-kitchen-ham_files.txt', '289-kitchen-ham_files.txt', '95-kitchen-ham_files.txt', '88-kitchen-ham_files.txt', '156-kitchen-ham_files.txt', '165-kitchen-ham_files.txt', '50-kitchen-ham_files.txt', '103-kitchen-ham_files.txt', '87-kitchen-ham_files.txt', '175-kitchen-ham_files.txt', '38-kitchen-ham_files.txt', '168-kitchen-ham_files.txt', '158-kitchen-ham_files.txt', '120-kitchen-ham_files.txt', '173-kitchen-ham_files.txt', '232-kitchen-ham_files.txt', '249-kitchen-ham_files.txt', '119-kitchen-ham_files.txt', '194-kitchen-ham_files.txt', '192-kitchen-ham_files.txt', '243-kitchen-ham_files.txt', '268-kitchen-ham_files.txt', '246-kitchen-ham_files.txt', '5-kitchen-ham_files.txt', '264-kitchen-ham_files.txt', '76-kitchen-ham_files.txt', '205-kitchen-ham_files.txt', '124-kitchen-ham_files.txt', '236-kitchen-ham_files.txt', '90-kitchen-ham_files.txt', '129-kitchen-ham_files.txt', '206-kitchen-ham_files.txt', '227-kitchen-ham_files.txt', '239-kitchen-ham_files.txt', '256-kitchen-ham_files.txt', '299-kitchen-ham_files.txt', '255-kitchen-ham_files.txt', '83-kitchen-ham_files.txt', '224-kitchen-ham_files.txt', '292-kitchen-ham_files.txt', '285-kitchen-ham_files.txt', '1-kitchen-ham_files.txt', '250-kitchen-ham_files.txt', '148-kitchen-ham_files.txt', '184-kitchen-ham_files.txt', '68-kitchen-ham_files.txt', '136-kitchen-ham_files.txt', '276-kitchen-ham_files.txt', '14-kitchen-ham_files.txt', '157-kitchen-ham_files.txt', '174-kitchen-ham_files.txt', '189-kitchen-ham_files.txt', '111-kitchen-ham_files.txt', '259-kitchen-ham_files.txt', '237-kitchen-ham_files.txt', '91-kitchen-ham_files.txt', '55-kitchen-ham_files.txt', '280-kitchen-ham_files.txt', '23-kitchen-ham_files.txt', '57-kitchen-ham_files.txt', '22-kitchen-ham_files.txt', '85-kitchen-ham_files.txt', '231-kitchen-ham_files.txt', '265-kitchen-ham_files.txt', '160-kitchen-ham_files.txt', '209-kitchen-ham_files.txt', '277-kitchen-ham_files.txt', '195-kitchen-ham_files.txt', '219-kitchen-ham_files.txt', '44-kitchen-ham_files.txt', '80-kitchen-ham_files.txt', '169-kitchen-ham_files.txt', '244-kitchen-ham_files.txt', '72-kitchen-ham_files.txt', '262-kitchen-ham_files.txt', '225-kitchen-ham_files.txt', '109-kitchen-ham_files.txt', '43-kitchen-ham_files.txt', '92-kitchen-ham_files.txt', '176-kitchen-ham_files.txt', '2-kitchen-ham_files.txt', '99-kitchen-ham_files.txt', '271-kitchen-ham_files.txt', '211-kitchen-ham_files.txt', '117-kitchen-ham_files.txt', '267-kitchen-ham_files.txt', '274-kitchen-ham_files.txt', '281-kitchen-ham_files.txt', '94-kitchen-ham_files.txt', '20-kitchen-ham_files.txt', '114-kitchen-ham_files.txt', '116-kitchen-ham_files.txt', '56-kitchen-ham_files.txt', '33-kitchen-ham_files.txt', '13-kitchen-ham_files.txt', '51-kitchen-ham_files.txt', '25-kitchen-ham_files.txt', '279-kitchen-ham_files.txt', '78-kitchen-ham_files.txt', '35-kitchen-ham_files.txt', '253-kitchen-ham_files.txt', '298-kitchen-ham_files.txt', '270-kitchen-ham_files.txt', '84-kitchen-ham_files.txt'] <class 'list'> These are all of the files containing non-spam emails, we will put them in a list, apply preprocessing techniques, and then add them to our data \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "import glob\n",
        "KitchenHam = os.getcwd()\n",
        "print(glob.glob('*.txt'),type(glob.glob('*.txt')), 'These are all of the files containing non-spam emails, we will put them in a list, apply preprocessing techniques, and then add them to our data ')\n",
        "type(KitchenHam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMF4tNzw5wvF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    we will now be creating a list full of non-spam emails, and we will apply\n",
        "\n",
        "'''\n",
        "k1 = glob.glob('*.txt')\n",
        "k1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo60v8AgLI6J"
      },
      "outputs": [],
      "source": [
        "def convertListToStr(docs):\n",
        "  str_  = ''\n",
        "  for i in docs:\n",
        "    str_ += str(i)\n",
        "  # print(str_)\n",
        "  # return str_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2Ou12N65wsb"
      },
      "outputs": [],
      "source": [
        "def read_directory(list1, path):\n",
        "  ham_emails = []\n",
        "  \n",
        "  '''\n",
        "     \n",
        "     This function will read a whole folder containing non spam emails, extract the emails and append them to a list.\n",
        "     This list will have preprocessing techniques applied on them later.\n",
        "\n",
        "  '''\n",
        "  for file in range(len(list1)):\n",
        "    temp_list = []\n",
        "    path_ = path + list1[file]\n",
        "    corpus = corpus = open(path_).read()\n",
        "    docs = corpus.split('\\n')\n",
        "    temp_list.append(docs)\n",
        "    temp_str = convertListToStr(temp_list)#.lower()\n",
        "\n",
        "    if temp_str == str:\n",
        "      print(True)\n",
        "    # ham_emails.append(temp_list)\n",
        "    for i in temp_list:\n",
        "      ham_emails.append(i)\n",
        "\n",
        "\n",
        "    print(corpus)\n",
        "  return ham_emails \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i4iFeTZ5wp0"
      },
      "outputs": [],
      "source": [
        "ham_emails = read_directory(k1, '/content/drive/MyDrive/Spam-mail-datasets/training_dataset_spammy/ham/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFVUeToW5wnM",
        "outputId": "4be3e58a-6137-460d-cd4a-2d0e8ed495e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ],
      "source": [
        "type(ham_emails[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ham_emails"
      ],
      "metadata": {
        "id": "k3ZPg7KWHLiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_ut2oOxNu6F"
      },
      "outputs": [],
      "source": [
        "ham_emails[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI5E40vg5wkp",
        "outputId": "f2389ad6-7e33-4050-edd0-d58fda5c1841"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "len(ham_emails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD4qw1VIhlrK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f5b453ad-26d5-4e6f-b411-137d7365aba3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    remove subject\\n    Subject:\\n    from :\\n    sent :\\n    date :\\n\\n    remove dates\\n    remove special characters\\n    remove cc:\\n    remove whitespaces\\n    remove emails\\n\\n    '- - - - - - - - - - - - - - - - - - - - - - forwarded by edith cross / hou / ect on 03 / 19 / 2001 10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -',\\n    ^^ remove that till the next time it encounters -- or till the end and make it a new entry in the list\\n    or from \\n\\n\\n    '- - - - - - - - - - - - - - - - - - - - - - forwarded by christopher f calger / pdx / ect on 03 / 25 / 2001 01 : 18 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -',\\n  'from : tim belden 03 / 21 / 2001 07 : 05 am',\\n  'to : louise kitchen / hou / ect @ ect',\\n  'cc : chris h foster / hou / ect @ ect , christopher f calger / pdx / ect @ ect',\\n  'subject : ees / epmi split',\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 250
        }
      ],
      "source": [
        "'''\n",
        "    remove subject\n",
        "    Subject:\n",
        "    from :\n",
        "    sent :\n",
        "    date :\n",
        "\n",
        "    remove dates\n",
        "    remove special characters\n",
        "    remove cc:\n",
        "    remove whitespaces\n",
        "    remove emails\n",
        "\n",
        "    '- - - - - - - - - - - - - - - - - - - - - - forwarded by edith cross / hou / ect on 03 / 19 / 2001 10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -',\n",
        "    ^^ remove that till the next time it encounters -- or till the end and make it a new entry in the list\n",
        "    or from \n",
        "\n",
        "\n",
        "    '- - - - - - - - - - - - - - - - - - - - - - forwarded by christopher f calger / pdx / ect on 03 / 25 / 2001 01 : 18 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -',\n",
        "  'from : tim belden 03 / 21 / 2001 07 : 05 am',\n",
        "  'to : louise kitchen / hou / ect @ ect',\n",
        "  'cc : chris h foster / hou / ect @ ect , christopher f calger / pdx / ect @ ect',\n",
        "  'subject : ees / epmi split',\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjkOUhiOhltu"
      },
      "outputs": [],
      "source": [
        "# ham_emails[9]\n",
        "ham_emails[165]\n",
        "# ham_emails[22]\n",
        "# ham_emails[10]\n",
        "# v = str(ham_emails[22]).strip()\n",
        "# v\n",
        "# ham_emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sN8g6k4-g8s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5de5b377-d9f1-4c5a-c737-1dac2649eacb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Subject: gas trading vision inception document'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 352
        }
      ],
      "source": [
        "ham_emails[2][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWXrXsaqhlwW",
        "outputId": "e19be932-d3d5-429b-cdc5-b908fd193a0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ham_emails[2][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3m9i4Cqhlzx"
      },
      "outputs": [],
      "source": [
        "ham_emails[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOqbH1cA5TMe"
      },
      "source": [
        "## Removing the following indexes from emails as this isn't really the body of the email, which we're going to feed to the model.   \n",
        "    'Subject:'\n",
        "    'from :'\n",
        "    'sent :'\n",
        "    'date :'\n",
        "## We will also be removing:\n",
        "    remove dates\n",
        "    remove special characters\n",
        "    remove cc:\n",
        "    remove whitespaces\n",
        "\n",
        "''' - - - - - - - - - - - - - - - - - - - - - - forwarded by edith cross / hou /ect on 03 / 19 / 2001 10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -'''\n",
        "\n",
        "## remove that till the next time it encounters -- or till the end and make it a new entry in the list, as this is showing a thread of valid emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD_90o01TVwP"
      },
      "outputs": [],
      "source": [
        "# sl = ['Subject:','from :','sent :','date :']\n",
        "# def removePhrases(lstr, pList):\n",
        "#   for word in lstr:\n",
        "#     for mail in pList:\n",
        "#       for e in mail:\n",
        "#         for j in e:\n",
        "#           if word in pList:\n",
        "#             e.remove(word)\n",
        "\n",
        "  # return pList\n",
        "# print(removePhrases(sl, ham_emails))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d-uXQsylOQw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkForwardedMails():\n",
        "  lst = [] #each email will be added in this list\n",
        "  dummy_str = \"- - -\" #this is the string which divides each mail\n",
        "  # new_mail_to_append = \"\" #the new mail will be added to this string and appended in lst.\n",
        "  indices_count = []\n",
        "  \n",
        "  for mail_count,mail in enumerate(ham_emails):\n",
        "    for index_count,e in enumerate(mail):\n",
        "      if dummy_str in e:\n",
        "        forwarded_mails = ham_emails[mail_count][index_count:]\n",
        "        indices_count.append((mail_count, index_count))\n",
        "        lst.append(forwarded_mails)\n",
        "  # print(lst)\n",
        "  return (lst,indices_count)"
      ],
      "metadata": {
        "id": "sI-Ielblpjhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forwarded_mails = checkForwardedMails()\n",
        "print(forwarded_mails[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNqnQGaRXwwR",
        "outputId": "fc1e65e1-72d4-4d43-ba14-d7b079f83ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(3, 4), (6, 4), (8, 16), (9, 3), (9, 15), (10, 4), (11, 4), (21, 7), (23, 2), (23, 9), (30, 6), (33, 6), (34, 5), (38, 2), (39, 3), (41, 10), (42, 1), (42, 10), (52, 3), (56, 2), (59, 2), (64, 26), (74, 5), (77, 5), (81, 6), (85, 10), (86, 5), (87, 3), (87, 12), (89, 4), (89, 6), (93, 2), (96, 3), (98, 3), (99, 2), (104, 2), (104, 9), (106, 2), (106, 11), (110, 2), (119, 4), (119, 12), (119, 24), (119, 31), (121, 4), (123, 2), (126, 4), (130, 29), (132, 2), (135, 4), (135, 15), (138, 5), (140, 3), (140, 6), (140, 16), (140, 23), (140, 26), (142, 2), (142, 5), (143, 3), (150, 5), (151, 4), (151, 13), (151, 18), (152, 9), (156, 15), (157, 6), (162, 2), (162, 9), (163, 5), (165, 7), (174, 2), (177, 12), (177, 19), (179, 1), (180, 2), (180, 8), (181, 2), (182, 4), (183, 2), (184, 2), (187, 2), (188, 5), (191, 4), (191, 12), (191, 20), (192, 24), (196, 4), (196, 26), (200, 6), (202, 1), (207, 2), (208, 30), (208, 37), (209, 6), (210, 2), (216, 2), (217, 3), (220, 3), (224, 1), (228, 22), (231, 5), (235, 2), (239, 4), (239, 22), (242, 4), (244, 1), (244, 8), (251, 3), (256, 86), (257, 2), (264, 1), (268, 4), (271, 2), (274, 5), (274, 49), (274, 55), (278, 7), (279, 4), (280, 4), (284, 5), (284, 8), (285, 6), (285, 13), (286, 4), (286, 10), (287, 16), (287, 30), (287, 43), (291, 2), (291, 16), (291, 29), (294, 1), (298, 1), (298, 9), (298, 24)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(forwarded_mails[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhKxOKRpslRL",
        "outputId": "0a2bb2ad-a21a-4293-eb10-ee7b8aaffaf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in forwarded_mails[1]:\n",
        "  print(i,'j', j)"
      ],
      "metadata": {
        "id": "8T1KUUpuaypR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"[(3, 4), (6, 4), (8, 16), (9, 3), (9, 15), (10, 4), (11, 4), (21, 7), (23, 2), (23, 9), (30, 6), (33, 6), (34, 5), (38, 2), (39, 3), (41, 10), (42, 1), (42, 10),\n",
        " (52, 3), (56, 2), (59, 2), (64, 26), (74, 5), (77, 5), (81, 6), (85, 10), (86, 5), (87, 3), (87, 12), (89, 4), (89, 6), (93, 2), (96, 3), (98, 3), (99, 2), (104, 2),\n",
        "  (104, 9), (106, 2), (106, 11), (110, 2), (119, 4), (119, 12), (119, 24), (119, 31), (121, 4), (123, 2), (126, 4), (130, 29), (132, 2), (135, 4), (135, 15), (138, 5), \n",
        "  (140, 3), (140, 6), (140, 16), (140, 23), (140, 26), (142, 2), (142, 5), (143, 3), (150, 5), (151, 4), (151, 13), (151, 18), (152, 9), (156, 15), (157, 6), (162, 2), (162, 9),\n",
        "   (163, 5), (165, 7), (174, 2), (177, 12), (177, 19), (179, 1), (180, 2), (180, 8), (181, 2), (182, 4), (183, 2), (184, 2), (187, 2), (188, 5), (191, 4), (191, 12), (191, 20), \n",
        "   (192, 24), (196, 4), (196, 26), (200, 6), (202, 1), (207, 2), (208, 30), (208, 37), (209, 6), (210, 2), (216, 2), (217, 3), (220, 3), (224, 1), (228, 22), (231, 5), (235, 2), \n",
        "   (239, 4), (239, 22), (242, 4), (244, 1), (244, 8), (251, 3), (256, 86), (257, 2), (264, 1), (268, 4), (271, 2), (274, 5), (274, 49), (274, 55), (278, 7), (279, 4), (280, 4),\n",
        "    (284, 5), (284, 8), (285, 6), (285, 13), (286, 4), (286, 10), (287, 16), (287, 30), (287, 43), (291, 2), (291, 16), (291, 29), (294, 1), (298, 1), (298, 9), (298, 24)] \"\"\""
      ],
      "metadata": {
        "id": "QPYnfBUyV62g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_ForwardedEmails(forwarded_mails_indices):\n",
        "  mails_to_remove = []\n",
        "  last_iterant = -1\n",
        "  '''\n",
        "      This function will remove the forwarded emails from ham_emails\n",
        "  '''\n",
        "  for i,j in forwarded_mails_indices:\n",
        "    if i == last_iterant:\n",
        "      continue\n",
        "    mails_to_remove.append(ham_emails[i])\n",
        "    last_iterant = i\n",
        "\n",
        "  for mails in mails_to_remove:\n",
        "    if mails in ham_emails:\n",
        "      ham_emails.remove(mails)\n",
        "\n",
        "  return ham_emails \n",
        "  \n",
        "\n",
        "def Add_ForwardedEmails(forwarded_mails_indices):\n",
        "  emails_to_add = []\n",
        "  for i,j in forwarded_mails_indices:\n",
        "    emails_to_add.append(ham_emails[i][:j])\n",
        "  \n",
        "  return emails_to_add\n",
        "\n",
        "\n",
        "# Add_ForwardedEmails(forwarded_mails[1])    \n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "pT99mwQoan3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(added)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5KYIENdjmTs",
        "outputId": "00485067-761b-4f13-9bf9-aa6bf9a6ff88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "added = Add_ForwardedEmails(forwarded_mails[1])   \n",
        "added"
      ],
      "metadata": {
        "id": "yqUg5oVZjRQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_ham_emails = remove_ForwardedEmails(forwarded_mails[1])"
      ],
      "metadata": {
        "id": "ybvdlsked78_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ham_emails[119]\n",
        "for i in added:\n",
        "  updated_ham_emails.append(i)"
      ],
      "metadata": {
        "id": "4lLeBIj1Xv4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(updated_ham_emails) ## To apply further preprocessing on."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTgHl711gOZQ",
        "outputId": "f05e5f49-3bef-4052-a8b0-d86187f43bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "338"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqIcjaRp4-CL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu0if0mG49_o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWObPeIF499P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg2hRp1-496V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iesjNNBs493r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO0Pf4XL490S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5Qj8t7ed_mT"
      },
      "outputs": [],
      "source": [
        "# print(nltk.word_tokenize(sentence[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av033vDQfccc"
      },
      "outputs": [],
      "source": [
        "# for i in sentence[1]:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bReq_jlAoO0c"
      },
      "outputs": [],
      "source": [
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qm9wjtFt_iP"
      },
      "outputs": [],
      "source": [
        "# It may not be possible manually provide the corrent POS tag for every word for large texts\n",
        "# So, instead, we will find out the correct POS tag for each word, map it to the right input character that the WordnetLemmatizer accepts \n",
        "# and pass it as the second argument to lemmatize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZE4jp7tIM3s"
      },
      "outputs": [],
      "source": [
        "# file:///tmp/mozilla_mak0/preprints202109.0236.v1.pdf\n",
        "'''\n",
        "After pre-processing, these texts go through numerous\n",
        "feature extraction methods, such as word2vec, word n-gram, character n-gram, and\n",
        "a combination of variable length n-gram. Different machine learning techniques such\n",
        "as support vector machine (SVM), decision tree (DT), logistic regression (LR), and\n",
        "multinomial naıve bayes (MNB) \n",
        "\n",
        "email security protocol not a valid feature as major dataset is old\n",
        "\n",
        "NER-> extract\n",
        "Features Increase:\n",
        "  computing related terms\n",
        "  medication related terms\n",
        "  currency mentioned?\n",
        "  money mentioned count\n",
        "  mail_context\n",
        "  cash mentioned?\n",
        "  adult content terms\n",
        "  email length\n",
        "  medication/computing/currency term count\n",
        "  punctuation count?\n",
        "  'part of speech count in row'\n",
        "  forward links given count\n",
        "  forward email- categorical\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAgfMHFUDTPm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJD-138Qdbds"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKeVEMxNdbap"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC_LrRBmdbXc"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTsIi7yndbUW"
      },
      "outputs": [],
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
        "               the world have come and invaded us, captured our lands, conquered our minds. \n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
        "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
        "               We have not grabbed their land, their culture, \n",
        "               their history and tried to enforce our way of life on them. \n",
        "               Why? Because we respect the freedom of others.That is why my \n",
        "               first vision is that of freedom. I believe that India got its first vision of \n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
        "               I see four milestones in my career\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkAG7arGdz8B"
      },
      "outputs": [],
      "source": [
        "paragraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90FzcLyhdbRp"
      },
      "outputs": [],
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "paragraph_ = paragraph.translate(str.maketrans('', '', PUNCT_TO_REMOVE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42nitM3_dmj5"
      },
      "outputs": [],
      "source": [
        "paragraph_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umy6xM9EdvPa"
      },
      "outputs": [],
      "source": [
        "# 5. Remove Stop Words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "paragraph_ = \" \".join([word for word in str(paragraph_).split() if word not in STOPWORDS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwdhfYlEfcPK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOv7OsW2giEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bb7360-5aa9-44cf-87de-a142bd16617d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 20, 30, 40, 50]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "nums = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "nums[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def l_():\n",
        "  for i in l:\n",
        "    for j in i:\n",
        "      for x in j:\n",
        "        if 'subject :' in x or 'from :' in x or 'sent :' in x or 'date :' in x or 'to :' in x or 'cc :' in x or 'cc' in x:\n",
        "          j.remove(x)\n",
        "\n",
        "        # return(x,j,True)\n",
        "  return l\n",
        "l = l_()"
      ],
      "metadata": {
        "id": "6LSPppXmxWJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lower(ham_emails):\n",
        "  for i in ham_emails:\n",
        "    for j in i:\n",
        "      for x in j:\n",
        "        x.lower()"
      ],
      "metadata": {
        "id": "VXOL0irlxeor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-mc10A2gtw4"
      },
      "outputs": [],
      "source": [
        "to_lower(l)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## USE STRING SLICING TO REMOVE ---------------------------"
      ],
      "metadata": {
        "id": "hFigImKpxHIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vITF6pm22uxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}